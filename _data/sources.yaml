- id: arxiv:1605.06409
  type: paper
  publisher: NeurIPS 2016
  image: images/1605.06409.png
  description: "<img src=\"https://img.shields.io/github/stars/daijifeng001/R-FCN\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">3rd of the most influential papers in NIPS 2016</button></a> <a href=\"https://pytorch.org/vision/main/generated/torchvision.ops.ps_roi_pool.html\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">Pytorch visual operator library</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1605.06409
    - type: source
      text: Code
      link: https://github.com/daijifeng001/R-FCN

- id: arxiv:2010.04159
  type: paper
  publisher: ICLR 2021 (Oral)
  image: images/2010.04159.png
  description: "<img src=\"https://img.shields.io/github/stars/fundamentalvision/Deformable-DETR\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://www.paperdigest.org/2022/02/most-influential-iclr-papers-2022-02/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">2nd of the most influential papers in ICLR 2021</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2010.04159
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/Deformable-DETR

- id: arxiv:1703.06211
  type: paper
  publisher: ICCV 2017 (Oral)
  image: images/1703.06211.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/Deformable-ConvNets\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://www.paperdigest.org/2022/05/most-influential-iccv-papers-2022-05/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">6th of the most influential papers in ICCV 2017</button></a> <a href=\"https://pytorch.org/vision/0.15/generated/torchvision.ops.deform_conv2d.html\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">Pytorch visual operator library</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1703.06211
    - type: source
      text: Code
      link: https://github.com/msracver/Deformable-ConvNets

- id: arxiv:2211.05778
  type: paper
  publisher: CVPR 2023 (Highlight)
  image: images/2211.05778.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2211.05778

- id: arxiv:1906.07155
  type: paper
  publisher: CVPR 2019
  image: images/1906.07155.png
  description: "<img src=\"https://img.shields.io/github/stars/open-mmlab/mmdetection\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1906.07155
    - type: source
      text: Code
      link: https://github.com/open-mmlab/mmdetection

- id: arxiv:1512.04412
  type: paper
  publisher: CVPR 2016 (Oral)
  image: images/1512.04412.png
  description: "<img src=\"https://img.shields.io/github/stars/daijifeng001/MNC\" style=\"margin:5px ;vertical-align: middle\"> <button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">COCO 2015 Segmentation Challenge Championship</button>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1512.04412
    - type: source
      text: Code
      link: https://github.com/daijifeng001/MNC

- id: arxiv:1811.11168
  type: paper
  publisher: CVPR 2019
  image: images/1811.11168.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/Deformable-ConvNets\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1811.11168
    - type: source
      text: Code
      link: https://github.com/msracver/Deformable-ConvNets

- id: arxiv:1711.11575
  type: paper
  publisher: CVPR 2018 (Oral)
  image: images/1711.11575.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/Relation-Networks-for-Object-Detection\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1711.11575
    - type: source
      text: Code
      link: https://github.com/msracver/Relation-Networks-for-Object-Detection

- id: arxiv:1611.07709
  type: paper
  publisher: CVPR 2017 (Spotlight)
  image: images/1611.07709.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/FCIS\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1611.07709
    - type: source
      text: Code
      link: https://github.com/msracver/FCIS

- id: arxiv:1503.01640
  type: paper
  publisher: ICCV 2015
  image: images/1503.01640.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1503.01640

- id: arxiv:1604.05144
  type: paper
  publisher: CVPR 2016 (Oral)
  image: images/1604.05144.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1604.05144
    - type: source
      text: Code
      link: http://www.jifengdai.org/downloads/scribble_sup/

- id: arxiv:1611.07715
  type: paper
  publisher: CVPR 2017
  image: images/1611.07715.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/Deep-Feature-Flow\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1611.07715
    - type: source
      text: Code
      link: https://github.com/msracver/Deep-Feature-Flow

- id: arxiv:1703.10025
  type: paper
  publisher: ICCV 2017
  image: images/1703.10025.png
  description: "<img src=\"https://img.shields.io/github/stars/msracver/Flow-Guided-Feature-Aggregation\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1703.10025
    - type: source
      text: Code
      link: https://github.com/msracver/Flow-Guided-Feature-Aggregation

- id: arxiv:1412.1283
  type: paper
  publisher: CVPR 2015
  image: images/1412.1283.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1412.1283

- id: arxiv:1908.08530
  type: paper
  publisher: ICLR 2020
  image: images/1908.08530.png
  description: "<img src=\"https://img.shields.io/github/stars/jackroos/VL-BERT\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://www.paperdigest.org/2022/02/most-influential-iclr-papers-2022-02/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">7th of the most influential papers in ICLR 2020</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1908.08530
    - type: source
      text: Code
      link: https://github.com/jackroos/VL-BERT

- id: arxiv:2112.01522
  type: paper
  publisher: CVPR 2022
  image: images/2112.01522.png
  description: "<img src=\"https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2112.01522
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/Uni-Perceiver

- id: arxiv:2206.04674
  type: paper
  publisher: NeurIPS 2022 (Spotlight)
  image: images/2206.04674.png
  description: "<img src=\"https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2206.04674
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/Uni-Perceiver

- id: arxiv:2211.09808
  type: paper
  publisher: CVPR 2023 (Highlight)
  image: images/2211.09808.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2211.09808

- id: arxiv:2305.11175
  type: paper
  publisher: Arxiv Tech Report 2023
  image: images/2305.11175.png
  description: "<img src=\"https://img.shields.io/github/stars/OpenGVLab/VisionLLM\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2305.11175
    - type: source
      text: Code
      link: https://github.com/OpenGVLab/VisionLLM

- id: arxiv:2203.17270
  type: paper
  publisher: ECCV 2022
  image: images/2203.17270.png
  description: "<img src=\"https://img.shields.io/github/stars/fundamentalvision/BEVFormer\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">6th of the most influential papers in ECCV 2022</button></a> <a href=\"https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">100 most cited AI papers of 2022</button></a> <a href=\"https://waymo.com/open/challenges/2022/3d-camera-only-detection/\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">Waymo 2022 Championship</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2203.17270
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/BEVFormer

- id: arxiv:2211.10439
  type: paper
  publisher: CVPR 2023 (Highlight)
  image: images/2211.10439.png
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2211.10439

- id: arxiv:2212.10156
  type: paper
  publisher: CVPR 2023 (Best Paper Award)
  image: images/2212.10156.png
  description: "<img src=\"https://img.shields.io/github/stars/opendrivelab/uniad\" style=\"margin:5px ;vertical-align: middle\"> <a href=\"https://cvpr2023.thecvf.com/Conferences/2023/Awards\"><button style=\"padding: 10px; display: inline-flex; font-family: var(--heading); font-weight: var(--semi-bold); background-color: #0ea5e9; color: white; border: none; border-radius: 10px; margin: 5px; gap: 10px; max-width: calc(100% - 5px - 5px); vertical-align: middle; cursor: pointer;\">Best Paper Award of CVPR 2023</button></a>"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2212.10156
    - type: source
      text: Code
      link: https://github.com/opendrivelab/uniad

- id: arxiv:2305.17144
  type: paper
  publisher: Arxiv Tech Report 2023
  image: images/2305.17144.png
  description: "<img src=\"https://img.shields.io/github/stars/OpenGVLab/GITM\" style=\"margin:5px ;vertical-align: middle\">"
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2305.17144
    - type: source
      text: Code
      link: https://github.com/OpenGVLab/GITM

# ------ Above are the highlight papers ------

- id: arxiv:2308.01907
  type: paper
  publisher: Arxiv Tech Report 2023
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2308.01907
    - type: source
      text: Code
      link: https://github.com/opengvlab/all-seeing

- id: arxiv:2211.05781
  type: paper
  publisher: Arxiv Tech Report 2022
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2211.05781
    - type: source
      text: Code
      link: https://github.com/opengvlab/stm-evaluation

- id: arxiv:2211.09807
  type: paper
  publisher: CVPR 2023 (Highlight)
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2211.09807
    - type: source
      text: Code
      link: https://github.com/OpenGVLab/M3I-Pretraining

- id: arxiv:2206.01204
  type: paper
  publisher: CVPR 2023 (Highlight)
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2206.01204
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/siamese-image-modeling

- id: arxiv:2111.13579
  type: paper
  publisher: ECCV 2022
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2111.13579
    - type: source
      text: Code
      link: https://github.com/ChangyaoTian/VL-LTR

- id: arxiv:2112.05141
  type: paper
  publisher: CVPR 2022
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2112.05141
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/unigrad

- id: arxiv:2103.14026
  type: paper
  publisher: CVPR 2022
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2103.14026

- id: https://openreview.net/forum?id=hLTZCN7f3M-
  type: paper
  publisher: NeurIPS 2021
  buttons:
    - type: paper
      link: https://openreview.net/forum?id=hLTZCN7f3M-
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/parameterized-ap-loss

- id: arxiv:2011.12953
  type: paper
  publisher: CVPR 2021
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2011.12953

- id: arxiv:2010.07930
  type: paper
  publisher: ICLR 2021
  buttons:
    - type: paper
      link: https://arxiv.org/abs/2010.07930
    - type: source
      text: Code
      link: https://github.com/fundamentalvision/Auto-Seg-Loss

- id: arxiv:1910.02940
  type: paper
  publisher: ICLR 2020
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1910.02940
    - type: source
      text: Code
      link: https://github.com/hangg7/deformable-kernels/

- id: arxiv:1904.05873
  type: paper
  publisher: ICCV 2019
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1904.05873

- id: arxiv:1811.11167
  type: paper
  publisher: Arxiv Tech Report 2018
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1811.11167

- id: arxiv:1804.05830
  type: paper
  publisher: Arxiv Tech Report 2018
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1804.05830

- id: arxiv:1803.07066
  type: paper
  publisher: ECCV 2018
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1803.07066

- id: arxiv:1603.08678
  type: paper
  publisher: ECCV 2016
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1603.08678

- id: arxiv:1412.6296
  type: paper
  publisher: ICLR 2015
  buttons:
    - type: paper
      link: https://arxiv.org/abs/1412.6296
    - type: source
      text: Code
      link: http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/main.html
