<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Team | Fundamental Vision</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Team">
<meta name="description" content="An engaging 1-3 sentence description of your lab.">

<meta property="og:title" content="Team">
<meta property="og:site_title" content="Fundamental Vision">
<meta property="og:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Team">
<meta property="twitter:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Team",
    "description": "An engaging 1-3 sentence description of your lab.",
    "headline": "Team",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-40 -60 80 100">
  <style>
    .bubble {
      animation: float 2s ease-out both infinite var(--delay);
    }
    @keyframes float {
      0% {
        opacity: 0;
      }
      50% {
        transform: translateY(0);
        opacity: 0;
      }
      75% {
        opacity: 1;
      }
      100% {
        opacity: 0;
        transform: translateY(-40px);
      }
    }
  </style>
  <g fill="currentColor" opacity="0.5">
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.1s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.4s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 1.1s"></circle>
  </g>
  <path fill="#38bdf8" d="
      M 0 -22.5
      L -19.5 -11.25
      L -19.5 11.25
      L 0 22.5
      L 19.5 11.25
      L 19.5 -11.25
      z
    "></path>
  <path fill="#bae6fd" d="
      M 0 -22.5
      L -19.5 -11.25
      L 0 0
      L 19.5 -11.25
      z
    "></path>
  <path fill="none" stroke="currentColor" stroke-width="5" d="
      M -18 -53
      L -10 -53
      L -10 -29.2
      L -30.3 -17.5
      L -30.3 17.5
      L 0 35
      L 30.3 17.5
      L 30.3 -17.5
      L 10 -29.2
      L 10 -53
      L 18 -53
    "></path>
</svg>

        
      </span>
    
    
      <span class="title" data-tooltip="Home">
        
          <span>Fundamental Vision</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/research/" data-tooltip="Research interests">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/blog/" data-tooltip="Musings and miscellany">
          Blog
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 id="team">
<i class="icon fa-solid fa-users"></i>Team</h1>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="faculties">Faculties</h2>

<div class="portrait-wrapper">
  <a href="/members/aaajifengdai.html" class="portrait" data-style="" aria-label="Jifeng Dai">
    <img src="/images/jifengdai.jpeg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Jifeng Dai
          </span>
        

        
          <span class="portrait-role">
            
            
            <span>Laboratory leader</span>
          </span>
        
      </span>
    
  </a>
</div>

<h2 id="phd-students">Ph.D Students</h2>

<div class="portrait-wrapper">
  <a href="/members/chenyuyang.html" class="portrait" data-style="" aria-label="Chenyu Yang">
    <img src="/images/chenyuyang.jpg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Chenyu Yang
          </span>
        

        
          <span class="portrait-role">
            
            <i class="icon fa-solid fa-graduation-cap"></i>
            <span>PhD Student</span>
          </span>
        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a href="/members/muyanzhong.html" class="portrait" data-style="" aria-label="Muyan Zhong">
    <img src="/images/muyanzhong.jpg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Muyan Zhong
          </span>
        

        
          <span class="portrait-role">
            
            <i class="icon fa-solid fa-graduation-cap"></i>
            <span>PhD Student</span>
          </span>
        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a href="/members/shiqiansu.html" class="portrait" data-style="" aria-label="Shiqian Su">
    <img src="/images/shiqiansu.jpg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Shiqian Su
          </span>
        

        
          <span class="portrait-role">
            
            <i class="icon fa-solid fa-graduation-cap"></i>
            <span>PhD Student</span>
          </span>
        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a href="/members/tiantongli.html" class="portrait" data-style="" aria-label="Tiantong Li">
    <img src="/images/tiantongli.jpg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Tiantong Li
          </span>
        

        
          <span class="portrait-role">
            
            <i class="icon fa-solid fa-graduation-cap"></i>
            <span>PhD Student</span>
          </span>
        
      </span>
    
  </a>
</div>

<h2 id="master-students">Master Students</h2>

<div class="portrait-wrapper">
  <a href="/members/senxing.html" class="portrait" data-style="" aria-label="Sen Xing">
    <img src="/images/senxing.jpg" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Sen Xing
          </span>
        

        
          <span class="portrait-role">
            
            
            <span>Master Student</span>
          </span>
        
      </span>
    
  </a>
</div>

<h2 id="other-students-and-interns">Other Students and Interns</h2>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Weijie Su">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Weijie Su
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Chenxin Tao">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Chenxin Tao
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Hao Li">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Hao Li
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Jinguo Zhu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Jinguo Zhu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Changyao Tian">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Changyao Tian
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Zhaokai Wang">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Zhaokai Wang
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Zhangwei Gao">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Zhangwei Gao
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yuchen Duan">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yuchen Duan
          </span>
        

        
      </span>
    
  </a>
</div>

<h2 id="former">Former</h2>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yi Li">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yi Li
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Haozhi Qi">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Haozhi Qi
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Xizhou Zhu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Xizhou Zhu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yuwen Xiong">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yuwen Xiong
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Di Lin">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Di Lin
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Jiayuan Gu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Jiayuan Gu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Bowen Cheng">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Bowen Cheng
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Dazhi Cheng">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Dazhi Cheng
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Hang Gao">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Hang Gao
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yinghao Xu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yinghao Xu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Zhenda Xie">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Zhenda Xie
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Xiaoshi Wu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Xiaoshi Wu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Jingqiu Zhou">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Jingqiu Zhou
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Haiyang Wang">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Haiyang Wang
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yuntao Chen">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yuntao Chen
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Tianwen Fu">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Tianwen Fu
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Hao Tian">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Hao Tian
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Yue Cao">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Yue Cao
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Zizhang Li">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Zizhang Li
          </span>
        

        
      </span>
    
  </a>
</div>

<div class="portrait-wrapper">
  <a class="portrait" data-style="small" aria-label="Zhiqi Li">
    <img src="" class="portrait-image" alt="member portrait" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">

    
      <span class="portrait-text">
        
          <span class="portrait-name">
            Zhiqi Li
          </span>
        

        
      </span>
    
  </a>
</div>
  </section>

  
  
  

  <section class="background" data-size="2" data-dark="true" style="--image: url('/images/background.jpg')">
    <!--
  background: images/background.jpg;
  dark: true;
  size: 2;
-->

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="grid" data-style="square">
  <style>
    /* Style for the rounded rectangle */
    .rounded-rectangle {
        display: inline-block;
        padding: 10px;
        border-radius: 10px; /* Adjust the border-radius to control the roundness */
        background-color: #4CAF50; /* Background color of the rectangle */
        text-align: center; /* Center the text inside the rectangle */
    }
</style>

<h1 id="publications">
<i class="icon fa-solid fa-microscope"></i>Publications</h1>

<p>The laboratory has published more than 60 academic papers, and many papers represented by deformable convolution have had great influence in the field.</p>
  

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="highlighted">Highlighted</h2>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1605.06409" class="citation-image" aria-label="R-FCN: Object Detection via Region-based Fully Convolutional Networks">
      <img src="/images/1605.06409.png" alt="R-FCN: Object Detection via Region-based Fully Convolutional Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1605.06409" class="citation-title">
      R-FCN: Object Detection via Region-based Fully Convolutional Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Yi Li, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      NeurIPS 2016
        ·  
      22 Jun 2016
        ·  
      arxiv:1605.06409
    </div>

    
      
        <div class="citation-description">
          The 3-rd most influential paper in Neural Information Processing Systems (NIPS), 2016. Pytorch visual operator library.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1605.06409" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/daijifeng001/R-FCN" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2010.04159" class="citation-image" aria-label="Deformable DETR: Deformable Transformers for End-to-End Object Detection">
      <img src="/images/2010.04159.png" alt="Deformable DETR: Deformable Transformers for End-to-End Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2010.04159" class="citation-title">
      Deformable DETR: Deformable Transformers for End-to-End Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2021 (Oral)
        ·  
      19 Mar 2021
        ·  
      arxiv:2010.04159
    </div>

    
      
        <div class="citation-description">
          The 2-nd most influential paper in International Conference on Learning Representations (ICLR), 2021.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2010.04159" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Deformable-DETR" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1703.06211" class="citation-image" aria-label="Deformable Convolutional Networks">
      <img src="/images/1703.06211.png" alt="Deformable Convolutional Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1703.06211" class="citation-title">
      Deformable Convolutional Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei
    </div>

    <div class="citation-details">
      ICCV 2017 (Oral)
        ·  
      06 Jun 2017
        ·  
      arxiv:1703.06211
    </div>

    
      
        <div class="citation-description">
          The 6-th most influential paper in International Conference on Computer Vision (ICCV), 2017. Pytorch visual operator library.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1703.06211" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deformable-ConvNets" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.05778" class="citation-image" aria-label="InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions">
      <img src="/images/2211.05778.png" alt="InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.05778" class="citation-title">
      InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions
    </a>

    <div class="citation-authors" data-tooltip="Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao " tabindex="0">
      Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, ..., Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      18 Apr 2023
        ·  
      arxiv:2211.05778
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.05778" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1512.04412" class="citation-image" aria-label="Instance-aware Semantic Segmentation via Multi-task Network Cascades">
      <img src="/images/1512.04412.png" alt="Instance-aware Semantic Segmentation via Multi-task Network Cascades" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1512.04412" class="citation-title">
      Instance-aware Semantic Segmentation via Multi-task Network Cascades
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2016 (Oral)
        ·  
      15 Dec 2015
        ·  
      arxiv:1512.04412
    </div>

    
      
        <div class="citation-description">
          1-st place winner of the COCO 2015 segmentation challenge. (0.36 sec/image test speed using VGG-16 net)

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1512.04412" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/daijifeng001/MNC" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1811.11168" class="citation-image" aria-label="Deformable ConvNets v2: More Deformable, Better Results">
      <img src="/images/1811.11168.png" alt="Deformable ConvNets v2: More Deformable, Better Results" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1811.11168" class="citation-title">
      Deformable ConvNets v2: More Deformable, Better Results
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2019
        ·  
      29 Nov 2018
        ·  
      arxiv:1811.11168
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1811.11168" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deformable-ConvNets" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1711.11575" class="citation-image" aria-label="Relation Networks for Object Detection">
      <img src="/images/1711.11575.png" alt="Relation Networks for Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1711.11575" class="citation-title">
      Relation Networks for Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2018 (Oral)
        ·  
      15 Jun 2018
        ·  
      arxiv:1711.11575
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1711.11575" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Relation-Networks-for-Object-Detection" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1611.07709" class="citation-image" aria-label="Fully Convolutional Instance-aware Semantic Segmentation">
      <img src="/images/1611.07709.png" alt="Fully Convolutional Instance-aware Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1611.07709" class="citation-title">
      Fully Convolutional Instance-aware Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2017 (Spotlight)
        ·  
      11 Apr 2017
        ·  
      arxiv:1611.07709
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1611.07709" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/FCIS" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1503.01640" class="citation-image" aria-label="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation">
      <img src="/images/1503.01640.png" alt="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1503.01640" class="citation-title">
      BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      ICCV 2015
        ·  
      19 May 2015
        ·  
      arxiv:1503.01640
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1503.01640" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1604.05144" class="citation-image" aria-label="ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation">
      <img src="/images/1604.05144.png" alt="ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1604.05144" class="citation-title">
      ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2016 (Oral)
        ·  
      19 Apr 2016
        ·  
      arxiv:1604.05144
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1604.05144" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="http://www.jifengdai.org/downloads/scribble_sup/" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1611.07715" class="citation-image" aria-label="Deep Feature Flow for Video Recognition">
      <img src="/images/1611.07715.png" alt="Deep Feature Flow for Video Recognition" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1611.07715" class="citation-title">
      Deep Feature Flow for Video Recognition
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2017
        ·  
      06 Jun 2017
        ·  
      arxiv:1611.07715
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1611.07715" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deep-Feature-Flow" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1703.10025" class="citation-image" aria-label="Flow-Guided Feature Aggregation for Video Object Detection">
      <img src="/images/1703.10025.png" alt="Flow-Guided Feature Aggregation for Video Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1703.10025" class="citation-title">
      Flow-Guided Feature Aggregation for Video Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei
    </div>

    <div class="citation-details">
      ICCV 2017
        ·  
      21 Aug 2017
        ·  
      arxiv:1703.10025
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1703.10025" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Flow-Guided-Feature-Aggregation" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1412.1283" class="citation-image" aria-label="Convolutional Feature Masking for Joint Object and Stuff Segmentation">
      <img src="/images/1412.1283.png" alt="Convolutional Feature Masking for Joint Object and Stuff Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1412.1283" class="citation-title">
      Convolutional Feature Masking for Joint Object and Stuff Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2015
        ·  
      18 Nov 2016
        ·  
      arxiv:1412.1283
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1412.1283" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1908.08530" class="citation-image" aria-label="VL-BERT: Pre-training of Generic Visual-Linguistic Representations">
      <img src="/images/1908.08530.png" alt="VL-BERT: Pre-training of Generic Visual-Linguistic Representations" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1908.08530" class="citation-title">
      VL-BERT: Pre-training of Generic Visual-Linguistic Representations
    </a>

    <div class="citation-authors" tabindex="0">
      Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2020
        ·  
      19 Feb 2020
        ·  
      arxiv:1908.08530
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1908.08530" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/jackroos/VL-BERT" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2112.01522" class="citation-image" aria-label="Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks">
      <img src="/images/2112.01522.png" alt="Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2112.01522" class="citation-title">
      Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2022
        ·  
      03 Dec 2021
        ·  
      arxiv:2112.01522
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2112.01522" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Uni-Perceiver" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2206.04674" class="citation-image" aria-label="Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs">
      <img src="/images/2206.04674.png" alt="Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2206.04674" class="citation-title">
      Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs
    </a>

    <div class="citation-authors" tabindex="0">
      Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      NeurIPS 2022 (Spotlight)
        ·  
      06 Jul 2022
        ·  
      arxiv:2206.04674
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2206.04674" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Uni-Perceiver" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.09808" class="citation-image" aria-label="Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks">
      <img src="/images/2211.09808.png" alt="Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.09808" class="citation-title">
      Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks
    </a>

    <div class="citation-authors" data-tooltip="Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai " tabindex="0">
      Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, ..., Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      18 Nov 2022
        ·  
      arxiv:2211.09808
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.09808" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2305.11175" class="citation-image" aria-label="VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks">
      <img src="/images/2305.11175.png" alt="VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2305.11175" class="citation-title">
      VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks
    </a>

    <div class="citation-authors" data-tooltip="Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai " tabindex="0">
      Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, ..., Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2023
        ·  
      26 May 2023
        ·  
      arxiv:2305.11175
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2305.11175" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/OpenGVLab/VisionLLM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2203.17270" class="citation-image" aria-label="BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
      <img src="/images/2203.17270.png" alt="BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2203.17270" class="citation-title">
      BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers
    </a>

    <div class="citation-authors" tabindex="0">
      Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai
    </div>

    <div class="citation-details">
      ECCV 2022
        ·  
      14 Jul 2022
        ·  
      arxiv:2203.17270
    </div>

    
      
        <div class="citation-description">
          The 6-th most influential paper in European Conference on Computer Vision (ECCV), 2022. The 100 most cited AI papers of 2022. Automatic-driving competition Waymo 2022 champion.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2203.17270" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/zhiqi-li/BEVFormer" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.10439" class="citation-image" aria-label="BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision">
      <img src="/images/2211.10439.png" alt="BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.10439" class="citation-title">
      BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision
    </a>

    <div class="citation-authors" data-tooltip="Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai " tabindex="0">
      Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, ..., Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      21 Nov 2022
        ·  
      arxiv:2211.10439
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.10439" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2212.10156" class="citation-image" aria-label="Planning-oriented Autonomous Driving">
      <img src="/images/2212.10156.png" alt="Planning-oriented Autonomous Driving" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2212.10156" class="citation-title">
      Planning-oriented Autonomous Driving
    </a>

    <div class="citation-authors" data-tooltip="Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li " tabindex="0">
      Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, ..., Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li
    </div>

    <div class="citation-details">
      CVPR 2023 (Best Paper Award)
        ·  
      24 Mar 2023
        ·  
      arxiv:2212.10156
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2212.10156" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/opendrivelab/uniad" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2305.17144" class="citation-image" aria-label="Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory">
      <img src="/images/2305.17144.png" alt="Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2305.17144" class="citation-title">
      Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory
    </a>

    <div class="citation-authors" data-tooltip="Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai " tabindex="0">
      Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, ..., Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2023
        ·  
      02 Jun 2023
        ·  
      arxiv:2305.17144
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2305.17144" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/OpenGVLab/GITM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/c7np" class="citation-image" aria-label="Open collaborative writing with Manubot">
      <img src="/images/different_items.gif" alt="Open collaborative writing with Manubot" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://doi.org/c7np" class="citation-title">
      Open collaborative writing with Manubot
    </a>

    <div class="citation-authors" tabindex="0">
      Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter
    </div>

    <div class="citation-details">
      PLOS Computational Biology
        ·  
      04 Dec 2020
        ·  
      doi:10.1371/journal.pcbi.1007128
    </div>

    
      
        <div class="citation-description">
          test description. <img src="https://img.shields.io/github/stars/daijifeng001/R-FCN" alt="GitHub Repo stars"> test stars. <a href="https://www.baidu.com"><button style="padding':' 10px 20px; background-color':' '#'4CAF50; color':' white; border':' none; border-radius':' 20px; cursor':' pointer;">点击我</button></a>

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://greenelab.github.io/meta-review/" data-tooltip="Manubot" data-style="bare" aria-label="Manubot">
      <span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewbox="-450 -250 900 500">  <g transform="scale(0.9)">    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        M -125 125         L -125 -125        L 0 45        L 125 -125        L 125 125      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        M -200 -250        L -330 0         L -200 250      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        m 200 250         L 330 0        L 200 -250      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="50" d="        M -330 0        L -370 0      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="50" d="        M 330 0        L 370 0      "></path>    <circle cx="-455" cy="0" r="30" fill="none" stroke="currentColor" stroke-width="30"></circle>    <circle cx="455" cy="0" r="30" fill="none" stroke="currentColor" stroke-width="30"></circle>  </g></svg></span>
      
        <span>Manubot</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/greenelab/meta-review" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Manuscript Source</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="http://manubot.org/" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>Website</span>
      
    </a>
  </div>


          
        </div>
      

      
        


  <div class="tags" data-repo="greenelab/meta-review" data-link="/publications/">
    
      <a href="/publications/?search=%22tag:%20open%20science%22" class="tag" data-tooltip='Show items with the tag "open science"'>
        open science
      </a>
    
      <a href="/publications/?search=%22tag:%20collaboration%22" class="tag" data-tooltip='Show items with the tag "collaboration"'>
        collaboration
      </a>
    
  </div>


      
    
  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="all">All</h2>

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="search-info"></div>

<h3 id="2023">2023</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2308.01907" class="citation-image" aria-label="The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World">
      <img src="" alt="The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2308.01907" class="citation-title">
      The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World
    </a>

    <div class="citation-authors" data-tooltip="Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, Yu Qiao " tabindex="0">
      Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, ..., Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, Yu Qiao
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2023
        ·  
      04 Aug 2023
        ·  
      arxiv:2308.01907
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2308.01907" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/opengvlab/all-seeing" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2305.17144" class="citation-image" aria-label="Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory">
      <img src="/images/2305.17144.png" alt="Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2305.17144" class="citation-title">
      Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory
    </a>

    <div class="citation-authors" data-tooltip="Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai " tabindex="0">
      Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, ..., Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2023
        ·  
      02 Jun 2023
        ·  
      arxiv:2305.17144
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2305.17144" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/OpenGVLab/GITM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2305.11175" class="citation-image" aria-label="VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks">
      <img src="/images/2305.11175.png" alt="VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2305.11175" class="citation-title">
      VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks
    </a>

    <div class="citation-authors" data-tooltip="Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai " tabindex="0">
      Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, ..., Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2023
        ·  
      26 May 2023
        ·  
      arxiv:2305.11175
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2305.11175" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/OpenGVLab/VisionLLM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.05778" class="citation-image" aria-label="InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions">
      <img src="/images/2211.05778.png" alt="InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.05778" class="citation-title">
      InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions
    </a>

    <div class="citation-authors" data-tooltip="Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao " tabindex="0">
      Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, ..., Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      18 Apr 2023
        ·  
      arxiv:2211.05778
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.05778" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/gr5hb5" class="citation-image" aria-label="MyGeneset.info: an interactive and programmatic platform for community-curated and user-created collections of genes">
      <img src="" alt="MyGeneset.info: an interactive and programmatic platform for community-curated and user-created collections of genes" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    

    <a href="https://doi.org/gr5hb5" class="citation-title">
      MyGeneset.info: an interactive and programmatic platform for community-curated and user-created collections of genes
    </a>

    <div class="citation-authors" tabindex="0">
      Ricardo Avila, Vincent Rubinetti, Xinghua Zhou, Dongbo Hu, Zhongchao Qian, Marco Alvarado Cano, Everaldo Rodolpho, Ginger Tsueng, Casey Greene, Chunlei Wu
    </div>

    <div class="citation-details">
      Nucleic Acids Research
        ·  
      18 Apr 2023
        ·  
      doi:10.1093/nar/gkad289
    </div>

    
      

      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2212.10156" class="citation-image" aria-label="Planning-oriented Autonomous Driving">
      <img src="/images/2212.10156.png" alt="Planning-oriented Autonomous Driving" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2212.10156" class="citation-title">
      Planning-oriented Autonomous Driving
    </a>

    <div class="citation-authors" data-tooltip="Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li " tabindex="0">
      Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, ..., Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li
    </div>

    <div class="citation-details">
      CVPR 2023 (Best Paper Award)
        ·  
      24 Mar 2023
        ·  
      arxiv:2212.10156
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2212.10156" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/opendrivelab/uniad" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/grmcb9" class="citation-image" aria-label="Hetnet connectivity search provides rapid insights into how two biomedical entities are related">
      <img src="" alt="Hetnet connectivity search provides rapid insights into how two biomedical entities are related" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    

    <a href="https://doi.org/grmcb9" class="citation-title">
      Hetnet connectivity search provides rapid insights into how two biomedical entities are related
    </a>

    <div class="citation-authors" data-tooltip="Daniel S. Himmelstein, Michael Zietz, Vincent Rubinetti, Kyle Kloster, Benjamin J. Heil, Faisal Alquaddoomi, Dongbo Hu, David N. Nicholson, Yun Hao, Blair D. Sullivan, Michael W. Nagle, Casey S. Greene " tabindex="0">
      Daniel S. Himmelstein, Michael Zietz, Vincent Rubinetti, Kyle Kloster, Benjamin J. Heil, ..., David N. Nicholson, Yun Hao, Blair D. Sullivan, Michael W. Nagle, Casey S. Greene
    </div>

    <div class="citation-details">
      Cold Spring Harbor Laboratory
        ·  
      07 Jan 2023
        ·  
      doi:10.1101/2023.01.05.522941
    </div>

    
      

      

      
    
  </div>
</div>

<h3 id="2022">2022</h3>

<div class="citation">
  
    <a href="https://doi.org/gsd85n" class="citation-image" aria-label="Hetnet connectivity search provides rapid insights into how biomedical entities are related">
      <img src="" alt="Hetnet connectivity search provides rapid insights into how biomedical entities are related" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    

    <a href="https://doi.org/gsd85n" class="citation-title">
      Hetnet connectivity search provides rapid insights into how biomedical entities are related
    </a>

    <div class="citation-authors" data-tooltip="Daniel S Himmelstein, Michael Zietz, Vincent Rubinetti, Kyle Kloster, Benjamin J Heil, Faisal Alquaddoomi, Dongbo Hu, David N Nicholson, Yun Hao, Blair D Sullivan, Michael W Nagle, Casey S Greene " tabindex="0">
      Daniel S Himmelstein, Michael Zietz, Vincent Rubinetti, Kyle Kloster, Benjamin J Heil, ..., David N Nicholson, Yun Hao, Blair D Sullivan, Michael W Nagle, Casey S Greene
    </div>

    <div class="citation-details">
      GigaScience
        ·  
      28 Dec 2022
        ·  
      doi:10.1093/gigascience/giad047
    </div>

    
      

      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.09807" class="citation-image" aria-label="Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information">
      <img src="" alt="Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.09807" class="citation-title">
      Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information
    </a>

    <div class="citation-authors" tabindex="0">
      Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      22 Nov 2022
        ·  
      arxiv:2211.09807
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.09807" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/OpenGVLab/M3I-Pretraining" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.10439" class="citation-image" aria-label="BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision">
      <img src="/images/2211.10439.png" alt="BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.10439" class="citation-title">
      BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision
    </a>

    <div class="citation-authors" data-tooltip="Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai " tabindex="0">
      Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, ..., Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      21 Nov 2022
        ·  
      arxiv:2211.10439
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.10439" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.09808" class="citation-image" aria-label="Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks">
      <img src="/images/2211.09808.png" alt="Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.09808" class="citation-title">
      Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks
    </a>

    <div class="citation-authors" data-tooltip="Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai " tabindex="0">
      Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, ..., Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      18 Nov 2022
        ·  
      arxiv:2211.09808
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.09808" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2206.01204" class="citation-image" aria-label="Siamese Image Modeling for Self-Supervised Vision Representation Learning">
      <img src="" alt="Siamese Image Modeling for Self-Supervised Vision Representation Learning" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2206.01204" class="citation-title">
      Siamese Image Modeling for Self-Supervised Vision Representation Learning
    </a>

    <div class="citation-authors" tabindex="0">
      Chenxin Tao, Xizhou Zhu, Weijie Su, Gao Huang, Bin Li, Jie Zhou, Yu Qiao, Xiaogang Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2023 (Highlight)
        ·  
      17 Nov 2022
        ·  
      arxiv:2206.01204
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2206.01204" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/siamese-image-modeling" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2211.05781" class="citation-image" aria-label="Demystify Transformers &amp; Convolutions in Modern Image Deep Networks">
      <img src="" alt="Demystify Transformers &amp; Convolutions in Modern Image Deep Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2211.05781" class="citation-title">
      Demystify Transformers &amp; Convolutions in Modern Image Deep Networks
    </a>

    <div class="citation-authors" data-tooltip="Jifeng Dai, Min Shi, Weiyun Wang, Sitong Wu, Linjie Xing, Wenhai Wang, Xizhou Zhu, Lewei Lu, Jie Zhou, Xiaogang Wang, Yu Qiao, Xiaowei Hu " tabindex="0">
      Jifeng Dai, Min Shi, Weiyun Wang, Sitong Wu, Linjie Xing, ..., Lewei Lu, Jie Zhou, Xiaogang Wang, Yu Qiao, Xiaowei Hu
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2022
        ·  
      11 Nov 2022
        ·  
      arxiv:2211.05781
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2211.05781" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/opengvlab/stm-evaluation" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2111.13579" class="citation-image" aria-label="VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition">
      <img src="" alt="VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2111.13579" class="citation-title">
      VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition
    </a>

    <div class="citation-authors" tabindex="0">
      Changyao Tian, Wenhai Wang, Xizhou Zhu, Jifeng Dai, Yu Qiao
    </div>

    <div class="citation-details">
      ECCV 2022
        ·  
      20 Jul 2022
        ·  
      arxiv:2111.13579
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2111.13579" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/ChangyaoTian/VL-LTR" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2203.17270" class="citation-image" aria-label="BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
      <img src="/images/2203.17270.png" alt="BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2203.17270" class="citation-title">
      BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers
    </a>

    <div class="citation-authors" tabindex="0">
      Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai
    </div>

    <div class="citation-details">
      ECCV 2022
        ·  
      14 Jul 2022
        ·  
      arxiv:2203.17270
    </div>

    
      
        <div class="citation-description">
          The 6-th most influential paper in European Conference on Computer Vision (ECCV), 2022. The 100 most cited AI papers of 2022. Automatic-driving competition Waymo 2022 champion.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2203.17270" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/zhiqi-li/BEVFormer" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2112.05141" class="citation-image" aria-label="Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework">
      <img src="" alt="Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2112.05141" class="citation-title">
      Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework
    </a>

    <div class="citation-authors" tabindex="0">
      Chenxin Tao, Honghui Wang, Xizhou Zhu, Jiahua Dong, Shiji Song, Gao Huang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2022
        ·  
      06 Jul 2022
        ·  
      arxiv:2112.05141
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2112.05141" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/unigrad" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2206.04674" class="citation-image" aria-label="Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs">
      <img src="/images/2206.04674.png" alt="Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2206.04674" class="citation-title">
      Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs
    </a>

    <div class="citation-authors" tabindex="0">
      Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      NeurIPS 2022 (Spotlight)
        ·  
      06 Jul 2022
        ·  
      arxiv:2206.04674
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2206.04674" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Uni-Perceiver" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2021">2021</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2112.01522" class="citation-image" aria-label="Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks">
      <img src="/images/2112.01522.png" alt="Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2112.01522" class="citation-title">
      Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2022
        ·  
      03 Dec 2021
        ·  
      arxiv:2112.01522
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2112.01522" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Uni-Perceiver" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://openreview.net/forum?id=hLTZCN7f3M-" class="citation-image" aria-label="Searching Parameterized AP Loss for Object Detection">
      <img src="" alt="Searching Parameterized AP Loss for Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://openreview.net/forum?id=hLTZCN7f3M-" class="citation-title">
      Searching Parameterized AP Loss for Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Chenxin Tao, Zizhang Li, Xizhou Zhu, Gao Huang, Yong Liu, Jifeng Dai
    </div>

    <div class="citation-details">
      NeurIPS 2021
        ·  
      09 Nov 2021
        ·  
      https://openreview.net/forum?id=hLTZCN7f3M-
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://openreview.net/forum?id=hLTZCN7f3M-" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/parameterized-ap-loss" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2011.12953" class="citation-image" aria-label="Unsupervised Object Detection with LiDAR Clues">
      <img src="" alt="Unsupervised Object Detection with LiDAR Clues" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2011.12953" class="citation-title">
      Unsupervised Object Detection with LiDAR Clues
    </a>

    <div class="citation-authors" tabindex="0">
      Hao Tian, Yuntao Chen, Jifeng Dai, Zhaoxiang Zhang, Xizhou Zhu
    </div>

    <div class="citation-details">
      CVPR 2021
        ·  
      20 Apr 2021
        ·  
      arxiv:2011.12953
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2011.12953" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2103.14026" class="citation-image" aria-label="AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks">
      <img src="" alt="AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2103.14026" class="citation-title">
      AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks
    </a>

    <div class="citation-authors" tabindex="0">
      Hao Li, Tianwen Fu, Jifeng Dai, Hongsheng Li, Gao Huang, Xizhou Zhu
    </div>

    <div class="citation-details">
      CVPR 2022
        ·  
      26 Mar 2021
        ·  
      arxiv:2103.14026
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2103.14026" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2010.04159" class="citation-image" aria-label="Deformable DETR: Deformable Transformers for End-to-End Object Detection">
      <img src="/images/2010.04159.png" alt="Deformable DETR: Deformable Transformers for End-to-End Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2010.04159" class="citation-title">
      Deformable DETR: Deformable Transformers for End-to-End Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2021 (Oral)
        ·  
      19 Mar 2021
        ·  
      arxiv:2010.04159
    </div>

    
      
        <div class="citation-description">
          The 2-nd most influential paper in International Conference on Learning Representations (ICLR), 2021.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2010.04159" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Deformable-DETR" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2020">2020</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/2010.07930" class="citation-image" aria-label="Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation">
      <img src="" alt="Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/2010.07930" class="citation-title">
      Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2021
        ·  
      04 Dec 2020
        ·  
      arxiv:2010.07930
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2010.07930" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision/Auto-Seg-Loss" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/c7np" class="citation-image" aria-label="Open collaborative writing with Manubot">
      <img src="/images/different_items.gif" alt="Open collaborative writing with Manubot" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://doi.org/c7np" class="citation-title">
      Open collaborative writing with Manubot
    </a>

    <div class="citation-authors" tabindex="0">
      Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter
    </div>

    <div class="citation-details">
      PLOS Computational Biology
        ·  
      04 Dec 2020
        ·  
      doi:10.1371/journal.pcbi.1007128
    </div>

    
      
        <div class="citation-description">
          test description. <img src="https://img.shields.io/github/stars/daijifeng001/R-FCN" alt="GitHub Repo stars"> test stars. <a href="https://www.baidu.com"><button style="padding':' 10px 20px; background-color':' '#'4CAF50; color':' white; border':' none; border-radius':' 20px; cursor':' pointer;">点击我</button></a>

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://greenelab.github.io/meta-review/" data-tooltip="Manubot" data-style="bare" aria-label="Manubot">
      <span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewbox="-450 -250 900 500">  <g transform="scale(0.9)">    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        M -125 125         L -125 -125        L 0 45        L 125 -125        L 125 125      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        M -200 -250        L -330 0         L -200 250      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="50" d="        m 200 250         L 330 0        L 200 -250      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="50" d="        M -330 0        L -370 0      "></path>    <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="50" d="        M 330 0        L 370 0      "></path>    <circle cx="-455" cy="0" r="30" fill="none" stroke="currentColor" stroke-width="30"></circle>    <circle cx="455" cy="0" r="30" fill="none" stroke="currentColor" stroke-width="30"></circle>  </g></svg></span>
      
        <span>Manubot</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/greenelab/meta-review" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Manuscript Source</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="http://manubot.org/" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>Website</span>
      
    </a>
  </div>


          
        </div>
      

      
        


  <div class="tags" data-repo="greenelab/meta-review" data-link="/publications/">
    
      <a href="/publications/?search=%22tag:%20open%20science%22" class="tag" data-tooltip='Show items with the tag "open science"'>
        open science
      </a>
    
      <a href="/publications/?search=%22tag:%20collaboration%22" class="tag" data-tooltip='Show items with the tag "collaboration"'>
        collaboration
      </a>
    
  </div>


      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/gg2mjh" class="citation-image" aria-label="Compressing gene expression data using multiple latent space dimensionalities learns complementary biological representations">
      <img src="" alt="Compressing gene expression data using multiple latent space dimensionalities learns complementary biological representations" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    

    <a href="https://doi.org/gg2mjh" class="citation-title">
      Compressing gene expression data using multiple latent space dimensionalities learns complementary biological representations
    </a>

    <div class="citation-authors" tabindex="0">
      Gregory P. Way, Michael Zietz, Vincent Rubinetti, Daniel S. Himmelstein, Casey S. Greene
    </div>

    <div class="citation-details">
      Genome Biology
        ·  
      11 May 2020
        ·  
      doi:10.1186/s13059-020-02021-3
    </div>

    
      

      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1908.08530" class="citation-image" aria-label="VL-BERT: Pre-training of Generic Visual-Linguistic Representations">
      <img src="/images/1908.08530.png" alt="VL-BERT: Pre-training of Generic Visual-Linguistic Representations" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1908.08530" class="citation-title">
      VL-BERT: Pre-training of Generic Visual-Linguistic Representations
    </a>

    <div class="citation-authors" tabindex="0">
      Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2020
        ·  
      19 Feb 2020
        ·  
      arxiv:1908.08530
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1908.08530" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/jackroos/VL-BERT" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1910.02940" class="citation-image" aria-label="Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation">
      <img src="" alt="Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1910.02940" class="citation-title">
      Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation
    </a>

    <div class="citation-authors" tabindex="0">
      Hang Gao, Xizhou Zhu, Steve Lin, Jifeng Dai
    </div>

    <div class="citation-details">
      ICLR 2020
        ·  
      13 Feb 2020
        ·  
      arxiv:1910.02940
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1910.02940" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/hangg7/deformable-kernels/" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2019">2019</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1906.07155" class="citation-image" aria-label="MMDetection: Open MMLab Detection Toolbox and Benchmark">
      <img src="/images/1906.07155.png" alt="MMDetection: Open MMLab Detection Toolbox and Benchmark" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1906.07155" class="citation-title">
      MMDetection: Open MMLab Detection Toolbox and Benchmark
    </a>

    <div class="citation-authors" data-tooltip="Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua Lin " tabindex="0">
      Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, ..., Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua Lin
    </div>

    <div class="citation-details">
      CVPR 2019
        ·  
      18 Jun 2019
        ·  
      arxiv:1906.07155
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1906.07155" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/open-mmlab/mmdetection" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1904.05873" class="citation-image" aria-label="An Empirical Study of Spatial Attention Mechanisms in Deep Networks">
      <img src="" alt="An Empirical Study of Spatial Attention Mechanisms in Deep Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1904.05873" class="citation-title">
      An Empirical Study of Spatial Attention Mechanisms in Deep Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, Jifeng Dai
    </div>

    <div class="citation-details">
      ICCV 2019
        ·  
      15 Apr 2019
        ·  
      arxiv:1904.05873
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1904.05873" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://doi.org/gfxjxf" class="citation-image" aria-label="Sequential compression of gene expression across dimensionalities and methods reveals no single best method or dimensionality">
      <img src="" alt="Sequential compression of gene expression across dimensionalities and methods reveals no single best method or dimensionality" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    

    <a href="https://doi.org/gfxjxf" class="citation-title">
      Sequential compression of gene expression across dimensionalities and methods reveals no single best method or dimensionality
    </a>

    <div class="citation-authors" tabindex="0">
      Gregory P. Way, Michael Zietz, Vincent Rubinetti, Daniel S. Himmelstein, Casey S. Greene
    </div>

    <div class="citation-details">
      Cold Spring Harbor Laboratory
        ·  
      11 Mar 2019
        ·  
      doi:10.1101/573782
    </div>

    
      

      

      
    
  </div>
</div>

<h3 id="2018">2018</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1811.11168" class="citation-image" aria-label="Deformable ConvNets v2: More Deformable, Better Results">
      <img src="/images/1811.11168.png" alt="Deformable ConvNets v2: More Deformable, Better Results" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1811.11168" class="citation-title">
      Deformable ConvNets v2: More Deformable, Better Results
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai
    </div>

    <div class="citation-details">
      CVPR 2019
        ·  
      29 Nov 2018
        ·  
      arxiv:1811.11168
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1811.11168" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deformable-ConvNets" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1811.11167" class="citation-image" aria-label="Integrated Object Detection and Tracking with Tracklet-Conditioned Detection">
      <img src="" alt="Integrated Object Detection and Tracking with Tracklet-Conditioned Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1811.11167" class="citation-title">
      Integrated Object Detection and Tracking with Tracklet-Conditioned Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Zheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, Jifeng Dai
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2018
        ·  
      28 Nov 2018
        ·  
      arxiv:1811.11167
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1811.11167" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1711.11575" class="citation-image" aria-label="Relation Networks for Object Detection">
      <img src="/images/1711.11575.png" alt="Relation Networks for Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1711.11575" class="citation-title">
      Relation Networks for Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2018 (Oral)
        ·  
      15 Jun 2018
        ·  
      arxiv:1711.11575
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1711.11575" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Relation-Networks-for-Object-Detection" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1804.05830" class="citation-image" aria-label="Towards High Performance Video Object Detection for Mobiles">
      <img src="" alt="Towards High Performance Video Object Detection for Mobiles" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1804.05830" class="citation-title">
      Towards High Performance Video Object Detection for Mobiles
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Jifeng Dai, Xingchi Zhu, Yichen Wei, Lu Yuan
    </div>

    <div class="citation-details">
      Arxiv Tech Report 2018
        ·  
      17 Apr 2018
        ·  
      arxiv:1804.05830
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1804.05830" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1803.07066" class="citation-image" aria-label="Learning Region Features for Object Detection">
      <img src="" alt="Learning Region Features for Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1803.07066" class="citation-title">
      Learning Region Features for Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai
    </div>

    <div class="citation-details">
      ECCV 2018
        ·  
      20 Mar 2018
        ·  
      arxiv:1803.07066
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1803.07066" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2017">2017</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1703.10025" class="citation-image" aria-label="Flow-Guided Feature Aggregation for Video Object Detection">
      <img src="/images/1703.10025.png" alt="Flow-Guided Feature Aggregation for Video Object Detection" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1703.10025" class="citation-title">
      Flow-Guided Feature Aggregation for Video Object Detection
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei
    </div>

    <div class="citation-details">
      ICCV 2017
        ·  
      21 Aug 2017
        ·  
      arxiv:1703.10025
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1703.10025" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Flow-Guided-Feature-Aggregation" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1611.07715" class="citation-image" aria-label="Deep Feature Flow for Video Recognition">
      <img src="/images/1611.07715.png" alt="Deep Feature Flow for Video Recognition" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1611.07715" class="citation-title">
      Deep Feature Flow for Video Recognition
    </a>

    <div class="citation-authors" tabindex="0">
      Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2017
        ·  
      06 Jun 2017
        ·  
      arxiv:1611.07715
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1611.07715" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deep-Feature-Flow" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1703.06211" class="citation-image" aria-label="Deformable Convolutional Networks">
      <img src="/images/1703.06211.png" alt="Deformable Convolutional Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1703.06211" class="citation-title">
      Deformable Convolutional Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei
    </div>

    <div class="citation-details">
      ICCV 2017 (Oral)
        ·  
      06 Jun 2017
        ·  
      arxiv:1703.06211
    </div>

    
      
        <div class="citation-description">
          The 6-th most influential paper in International Conference on Computer Vision (ICCV), 2017. Pytorch visual operator library.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1703.06211" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/Deformable-ConvNets" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1611.07709" class="citation-image" aria-label="Fully Convolutional Instance-aware Semantic Segmentation">
      <img src="/images/1611.07709.png" alt="Fully Convolutional Instance-aware Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1611.07709" class="citation-title">
      Fully Convolutional Instance-aware Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei
    </div>

    <div class="citation-details">
      CVPR 2017 (Spotlight)
        ·  
      11 Apr 2017
        ·  
      arxiv:1611.07709
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1611.07709" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/msracver/FCIS" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2016">2016</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1412.1283" class="citation-image" aria-label="Convolutional Feature Masking for Joint Object and Stuff Segmentation">
      <img src="/images/1412.1283.png" alt="Convolutional Feature Masking for Joint Object and Stuff Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1412.1283" class="citation-title">
      Convolutional Feature Masking for Joint Object and Stuff Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2015
        ·  
      18 Nov 2016
        ·  
      arxiv:1412.1283
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1412.1283" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1605.06409" class="citation-image" aria-label="R-FCN: Object Detection via Region-based Fully Convolutional Networks">
      <img src="/images/1605.06409.png" alt="R-FCN: Object Detection via Region-based Fully Convolutional Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1605.06409" class="citation-title">
      R-FCN: Object Detection via Region-based Fully Convolutional Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Yi Li, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      NeurIPS 2016
        ·  
      22 Jun 2016
        ·  
      arxiv:1605.06409
    </div>

    
      
        <div class="citation-description">
          The 3-rd most influential paper in Neural Information Processing Systems (NIPS), 2016. Pytorch visual operator library.

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1605.06409" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/daijifeng001/R-FCN" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1604.05144" class="citation-image" aria-label="ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation">
      <img src="/images/1604.05144.png" alt="ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1604.05144" class="citation-title">
      ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2016 (Oral)
        ·  
      19 Apr 2016
        ·  
      arxiv:1604.05144
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1604.05144" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="http://www.jifengdai.org/downloads/scribble_sup/" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1603.08678" class="citation-image" aria-label="Instance-sensitive Fully Convolutional Networks">
      <img src="" alt="Instance-sensitive Fully Convolutional Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1603.08678" class="citation-title">
      Instance-sensitive Fully Convolutional Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun
    </div>

    <div class="citation-details">
      ECCV 2016
        ·  
      30 Mar 2016
        ·  
      arxiv:1603.08678
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1603.08678" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<h3 id="2015">2015</h3>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1512.04412" class="citation-image" aria-label="Instance-aware Semantic Segmentation via Multi-task Network Cascades">
      <img src="/images/1512.04412.png" alt="Instance-aware Semantic Segmentation via Multi-task Network Cascades" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1512.04412" class="citation-title">
      Instance-aware Semantic Segmentation via Multi-task Network Cascades
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      CVPR 2016 (Oral)
        ·  
      15 Dec 2015
        ·  
      arxiv:1512.04412
    </div>

    
      
        <div class="citation-description">
          1-st place winner of the COCO 2015 segmentation challenge. (0.36 sec/image test speed using VGG-16 net)

        </div>
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1512.04412" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="https://github.com/daijifeng001/MNC" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1503.01640" class="citation-image" aria-label="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation">
      <img src="/images/1503.01640.png" alt="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1503.01640" class="citation-title">
      BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Kaiming He, Jian Sun
    </div>

    <div class="citation-details">
      ICCV 2015
        ·  
      19 May 2015
        ·  
      arxiv:1503.01640
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1503.01640" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>

<div class="citation">
  
    <a href="https://arxiv.org/abs/1412.6296" class="citation-image" aria-label="Generative Modeling of Convolutional Neural Networks">
      <img src="" alt="Generative Modeling of Convolutional Neural Networks" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
    </a>
  

  <div class="citation-text">
    
    <i class="icon fa-solid fa-scroll"></i>

    <a href="https://arxiv.org/abs/1412.6296" class="citation-title">
      Generative Modeling of Convolutional Neural Networks
    </a>

    <div class="citation-authors" tabindex="0">
      Jifeng Dai, Yang Lu, Ying-Nian Wu
    </div>

    <div class="citation-details">
      ICLR 2015
        ·  
      10 Apr 2015
        ·  
      arxiv:1412.6296
    </div>

    
      

      
        <div class="citation-buttons">
          
            



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/1412.6296" data-tooltip="Paper" data-style="bare" aria-label="Paper">
      <i class="icon fa-solid fa-scroll"></i>
      
        <span>Paper</span>
      
    </a>
  </div>


          
            



  <div class="button-wrapper">
    <a class="button" href="http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/main.html" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>Code</span>
      
    </a>
  </div>


          
        </div>
      

      
    
  </div>
</div>


</section>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:daijifeng@tsinghua.edu.cn" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=SH_-B_AAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/fundamentalvision" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2023
    Fundamental Vision
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
